{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP Classifier Tutorial for Large Datasets\n",
    "\n",
    "UMAP and the attached prediction methods can get very memory consuming for large datasets. In this tutorial, we present solutions to that problem. If you haven't yet, consider taking a look at the [UMAP Classifier Tutorial](umap_classifier.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cr/work/kastner/miniconda3/envs/aera01/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-08-31 00:40:41.047614: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-31 00:40:41.063315: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756593641.079070 3042123 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756593641.083958 3042123 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756593641.096028 3042123 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756593641.096041 3042123 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756593641.096043 3042123 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756593641.096044 3042123 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-31 00:40:41.100039: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# import modules\n",
    "import sys\n",
    "sys.path.insert(0, '../../')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from helper_scripts import random_rm_trace_generator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc\n",
    "import importlib\n",
    "from matplotlib import pyplot as plt\n",
    "from modules import umap_classifier\n",
    "from modules import performance_metrics as pm\n",
    "import joblib\n",
    "import os\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Often, unfiltered monitoring datasets contain lots of \"normal\" events, which are not very interesting for the overall analysis and a few interesting events. Furthermore, they might be stored in separate files. Here, we present a method of loading the data from the files and filtering them with UMAP and DBSCAN/HDBSCAN on the go in order to maximize the Shannon entropy among the predicted cluster labels of the selected events. However, since this can be either very time or memory expensive, the prefered method should always be to filter the given datasets in advance. If the normal behavior is already known, this can be done for example by introducing likelihood cuts to preselect conspicious events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first define a custom data loader function, which loads the data from a given file path\n",
    "# this function has to return the data and interesting properties (either None or np.array(batch_size, n_properties)) \n",
    "# that shall be kept for later analysis\n",
    "# in this case, we want to keep the true cluster information for later performance evaluation\n",
    "\n",
    "def custom_data_loader(file_path):\n",
    "    df = joblib.load(file_path)\n",
    "    return np.stack(df[\"time_trace\"].to_numpy()), np.array([df[\"true_cluster\"].to_numpy()]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "# Load data batch-wise from directory and select interesting events\n",
    "dir_path = '../data/large_dataset_1'\n",
    "u_clas = umap_classifier.UMAPClassifier(yaml_path=\"../yaml_files/umap_classifier_test.yaml\", input_data_type=\"time\")\n",
    "selected_data, selected_properties = u_clas.load_umap_data_from_dir(dir_path, \n",
    "                            data_loader=custom_data_loader, # custom data loader function\n",
    "                            clustering_mode=\"hdbscan\", # clustering mode can be \"dbscan\" or \"hdbscan\"\n",
    "                            max_umap_events=2000, # maximum number of events for the final the UMAP model\n",
    "                            batch_size=5000, # number of events to be loaded and processed at once\n",
    "                            n_selected_per_batch=1000, # number of interesting events to be selected from each batch\n",
    "                            summary_batch_size=3000, # number of events after which the current best data selection is updated\n",
    "                            verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected data type: <class 'numpy.memmap'>\n",
      "Selected data shape: (2000, 512)\n",
      "Selected properties type: <class 'numpy.memmap'>\n",
      "Selected properties shape: (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# the data is returned in RAM efficient memory-mapped numpy arrays\n",
    "print(\"Selected data type:\",type(selected_data))\n",
    "print(\"Selected data shape:\",selected_data.shape)\n",
    "print(\"Selected properties type:\",type(selected_properties))\n",
    "print(\"Selected properties shape:\",selected_properties.shape if selected_properties is not None else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1, 0, 1, 2, 3, 4, 5, 6, 7, 8], dtype=object),\n",
       " array([701, 176, 142, 142, 139, 145, 139, 134, 142, 140]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The identified interesting events are\n",
    "# As one can see, many of the noise events (true_cluster = -1) are selected.\n",
    "# The other clusters are approximately uniformly distributed\n",
    "np.unique(selected_properties,return_counts=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data batch-wise**\n",
    "> For the case if reading the files already delivers huge datasets, we also offer an opportunitiy to already load the dta batch-wise from the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom data batch loader function which just loads a certain batch of data from a given file path\n",
    "# This function has to return the data batch, the interesting properties and a boolean variable indicating if\n",
    "# the last batch from the corresponding file has been reached (True if last batch, False otherwise)\n",
    "batch_size = 5000\n",
    "def custom_data_batch_loader(batch_idx,file_path):\n",
    "    df = joblib.load(file_path)\n",
    "    i_min = batch_idx*batch_size\n",
    "    i_max = min((batch_idx+1)*batch_size, len(df))\n",
    "    if i_max >= len(df):\n",
    "        return np.stack(df[\"time_trace\"].to_numpy())[i_min:i_max], np.array([df[\"true_cluster\"].to_numpy()]).T[i_min:i_max], True\n",
    "    else:\n",
    "        return np.stack(df[\"time_trace\"].to_numpy())[i_min:i_max], np.array([df[\"true_cluster\"].to_numpy()]).T[i_min:i_max], False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "\n",
    "# Load data batch-wise from directory and select interesting events\n",
    "dir_path = '../data/large_dataset_1'\n",
    "u_clas = umap_classifier.UMAPClassifier(yaml_path=\"../yaml_files/umap_classifier_test.yaml\", input_data_type=\"time\")\n",
    "u_clas.load_umap_data_batchwise_from_dir(dir_path, \n",
    "                            data_batch_loader=custom_data_batch_loader,\n",
    "                            clustering_mode=\"dbscan\",\n",
    "                            max_umap_events=3000,\n",
    "                            batch_size=5000,\n",
    "                            summary_batch_size=3000,\n",
    "                            n_selected_per_batch=1000,\n",
    "                            verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected data type: <class 'numpy.memmap'>\n",
      "Selected data shape: (2000, 512)\n",
      "Selected properties type: <class 'numpy.memmap'>\n",
      "Selected properties shape: (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# the data is returned in RAM efficient memory-mapped numpy arrays\n",
    "print(\"Selected data type:\",type(selected_data))\n",
    "print(\"Selected data shape:\",selected_data.shape)\n",
    "print(\"Selected properties type:\",type(selected_properties))\n",
    "print(\"Selected properties shape:\",selected_properties.shape if selected_properties is not None else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1, 0, 1, 2, 3, 4, 5, 6, 7, 8], dtype=object),\n",
       " array([437, 176, 175, 169, 177, 181, 179, 167, 175, 164]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The identified interesting events are\n",
    "# As one can see, many of the noise events (true_cluster = -1) are selected.\n",
    "# The other clusters are approximately uniformly distributed\n",
    "np.unique(selected_properties, return_counts=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aera01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
